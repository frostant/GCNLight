#######################
cuda:1
#######################
[0;30;43mCpp extension not loaded[0m
>>SEED: 2020
[0;30;43mloading [../data/gowalla][0m
self.edgeWeight: False
self.edgeV2: True
self.edgeLog: False
SHape (810128,)
810128 interactions for training
217242 interactions for testing
gowalla Sparsity : 0.0008396216228570436
gowalla is ready to go
===========config================
{'A_n_fold': 100,
 'A_split': False,
 'bigdata': False,
 'bpr_batch_size': 2048,
 'decay': 0.0005,
 'dropout': 0,
 'keep_prob': 0.5,
 'latent_dim_rec': 64,
 'lightGCN_n_layers': 1,
 'lr': 0.001,
 'multicore': 0,
 'pretrain': 0,
 'test_u_batch_size': 100}
cores for test: 16
comment: lgn
tensorboard: 1
LOAD: 0
Weight path: ./checkpoints
Test Topks: [20]
using bpr loss
===========end===================
[0;30;43muse NORMAL distribution initilizer[0m
loading adjacency matrix
29858
40981
(29858, 40981)
generating adjacency matrix
SDF==234===
SDF==12===
SDF=====
  (29858, 0)	225.0
  (29859, 0)	214.0
  (29860, 0)	247.0
  (29861, 0)	290.0
  (29862, 0)	181.0
  (29863, 0)	235.0
  (29864, 0)	106.0
  (29865, 0)	252.0
  (29866, 0)	185.0
  (29867, 0)	138.0
  (29868, 0)	32.0
  (29869, 0)	522.0
  (29870, 0)	227.0
  (29871, 0)	247.0
  (29872, 0)	158.0
  (29873, 0)	235.0
  (29874, 0)	269.0
  (29875, 0)	442.0
  (29876, 0)	854.0
  (29877, 0)	617.0
  (29878, 0)	175.0
  (29879, 0)	561.0
  (29880, 0)	810.0
  (29881, 0)	92.0
  (29882, 0)	78.0
  :	:
  (29733, 70829)	8.0
  (29825, 70829)	4.0
  (27924, 70830)	2.0
  (29361, 70830)	2.0
  (29492, 70830)	1.0
  (29590, 70830)	1.0
  (28382, 70831)	3.0
  (28744, 70831)	2.0
  (29023, 70831)	2.0
  (29040, 70831)	3.0
  (28570, 70832)	1.0
  (28650, 70833)	3.0
  (29065, 70833)	4.0
  (29133, 70833)	11.0
  (29340, 70833)	3.0
  (29396, 70833)	8.0
  (29597, 70833)	6.0
  (29759, 70833)	5.0
  (29842, 70833)	4.0
  (28752, 70834)	1.0
  (28885, 70835)	1.0
  (29135, 70836)	1.0
  (29322, 70837)	1.0
  (29527, 70837)	1.0
  (29371, 70838)	1.0
(70839, 70839)
SDF==start===
  (29858, 0)	1.0
  (29859, 0)	1.0
  (29860, 0)	1.0
  (29861, 0)	1.0
  (29862, 0)	1.0
  (29863, 0)	1.0
  (29864, 0)	1.0
  (29865, 0)	1.0
  (29866, 0)	1.0
  (29867, 0)	1.0
  (29868, 0)	1.0
  (29869, 0)	1.0
  (29870, 0)	1.0
  (29871, 0)	1.0
  (29872, 0)	1.0
  (29873, 0)	1.0
  (29874, 0)	1.0
  (29875, 0)	1.0
  (29876, 0)	1.0
  (29877, 0)	1.0
  (29878, 0)	1.0
  (29879, 0)	1.0
  (29880, 0)	1.0
  (29881, 0)	1.0
  (29882, 0)	1.0
  :	:
  (29733, 70829)	1.0
  (29825, 70829)	1.0
  (27924, 70830)	1.0
  (29361, 70830)	1.0
  (29492, 70830)	1.0
  (29590, 70830)	1.0
  (28382, 70831)	1.0
  (28744, 70831)	1.0
  (29023, 70831)	1.0
  (29040, 70831)	1.0
  (28570, 70832)	1.0
  (28650, 70833)	1.0
  (29065, 70833)	1.0
  (29133, 70833)	1.0
  (29340, 70833)	1.0
  (29396, 70833)	1.0
  (29597, 70833)	1.0
  (29759, 70833)	1.0
  (29842, 70833)	1.0
  (28752, 70834)	1.0
  (28885, 70835)	1.0
  (29135, 70836)	1.0
  (29322, 70837)	1.0
  (29527, 70837)	1.0
  (29371, 70838)	1.0
[[127.]
 [ 49.]
 [ 84.]
 ...
 [  1.]
 [  2.]
 [  1.]]
costing 122.674631357193s, saved norm_mat...
  (0, 29858)	5.5374393
  (0, 29859)	5.2667203
  (0, 29860)	5.166053
  (0, 29861)	9.098108
  (0, 29862)	5.3537173
  (0, 29863)	6.950959
  (0, 29864)	2.9744315
  (0, 29865)	5.976331
  (0, 29866)	5.472032
  (0, 29867)	2.04092
  (0, 29868)	0.8561537
  (0, 29869)	3.7325726
  (0, 29870)	3.404787
  (0, 29871)	2.3498247
  (0, 29872)	2.9891236
  (0, 29873)	3.6863027
  (0, 29874)	3.310158
  (0, 29875)	4.981092
  (0, 29876)	6.1264696
  (0, 29877)	5.4749894
  (0, 29878)	3.5625367
  (0, 29879)	5.949931
  (0, 29880)	6.0746202
  (0, 29881)	2.356651
  (0, 29882)	2.0868747
  :	:
  (70829, 29733)	0.76277006
  (70829, 29825)	0.4338609
  (70830, 27924)	0.2581989
  (70830, 29361)	0.28867513
  (70830, 29492)	0.14433756
  (70830, 29590)	0.14433756
  (70831, 28382)	0.36380345
  (70831, 28744)	0.20412415
  (70831, 29023)	0.20412415
  (70831, 29040)	0.35355338
  (70832, 28570)	0.25
  (70833, 28650)	0.19364916
  (70833, 29065)	0.31622776
  (70833, 29133)	0.74845517
  (70833, 29340)	0.35355338
  (70833, 29396)	0.78446454
  (70833, 29597)	0.42426404
  (70833, 29759)	0.49029034
  (70833, 29842)	0.24253562
  (70834, 28752)	0.16666667
  (70835, 28885)	0.12598816
  (70836, 29135)	0.33333334
  (70837, 29322)	0.24999999
  (70837, 29527)	0.24999999
  (70838, 29371)	0.26726124
(70839, 70839)
don't split the matrix
tensor(indices=tensor([[    0,     0,     0,  ..., 70837, 70837, 70838],
                       [29858, 29859, 29860,  ..., 29322, 29527, 29371]]),
       values=tensor([5.5374, 5.2667, 5.1661,  ..., 0.2500, 0.2500, 0.2673]),
       device='cuda:1', size=(70839, 70839), nnz=1620256,
       layout=torch.sparse_coo)
lgn is already to go(dropout:0)
num user: 29858
num item: 40981
graph: tensor(0., device='cuda:1')
load and save to ./light-gcn/code/checkpoints/lgn-gowalla-1-64.pth.tar
[0;30;43m[TEST][0m
{'precision': array([0.00125092]), 'recall': array([0.00374194]), 'ndcg': array([0.00244197])}
#####W######
5.69
[array([0.00374194]), array([0.00244197]), 0]
EPOCH[1/1500] loss0.614-|Sample:10.89|
[0;30;43m[TEST][0m
{'precision': array([0.01268169]), 'recall': array([0.05138332]), 'ndcg': array([0.03167181])}
#####W######
5.43
[array([0.05138332]), array([0.03167181]), 0]
EPOCH[11/1500] loss0.296-|Sample:10.77|
[0;30;43m[TEST][0m
{'precision': array([0.01313216]), 'recall': array([0.05195607]), 'ndcg': array([0.03120353])}
#####W######
5.48
[array([0.05195607]), array([0.03120353]), 0]
EPOCH[21/1500] loss0.303-|Sample:10.95|
[0;30;43m[TEST][0m
{'precision': array([0.01321924]), 'recall': array([0.05376264]), 'ndcg': array([0.03228145])}
#####W######
5.43
[array([0.05376264]), array([0.03228145]), 0]
EPOCH[31/1500] loss0.263-|Sample:10.71|
[0;30;43m[TEST][0m
{'precision': array([0.01366803]), 'recall': array([0.05488581]), 'ndcg': array([0.03304278])}
#####W######
5.44
[array([0.05488581]), array([0.03304278]), 0]
EPOCH[41/1500] loss0.273-|Sample:10.56|
[0;30;43m[TEST][0m
{'precision': array([0.0127738]), 'recall': array([0.05123461]), 'ndcg': array([0.03035224])}
#####W######
5.46
[array([0.05488581]), array([0.03304278]), 1]
EPOCH[51/1500] loss0.318-|Sample:10.58|
[0;30;43m[TEST][0m
{'precision': array([0.01305848]), 'recall': array([0.05265018]), 'ndcg': array([0.0313291])}
#####W######
5.51
[array([0.05488581]), array([0.03304278]), 2]
EPOCH[61/1500] loss0.289-|Sample:10.63|
[0;30;43m[TEST][0m
{'precision': array([0.01470963]), 'recall': array([0.05962146]), 'ndcg': array([0.03533152])}
#####W######
5.44
[array([0.05962146]), array([0.03533152]), 0]
EPOCH[71/1500] loss0.299-|Sample:10.75|
[0;30;43m[TEST][0m
{'precision': array([0.01339172]), 'recall': array([0.05457279]), 'ndcg': array([0.0321802])}
#####W######
5.45
[array([0.05962146]), array([0.03533152]), 1]
EPOCH[81/1500] loss0.281-|Sample:11.32|
[0;30;43m[TEST][0m
{'precision': array([0.01313551]), 'recall': array([0.05311113]), 'ndcg': array([0.03180219])}
#####W######
5.51
[array([0.05962146]), array([0.03533152]), 2]
EPOCH[91/1500] loss0.292-|Sample:10.80|
[0;30;43m[TEST][0m
{'precision': array([0.01156307]), 'recall': array([0.04626649]), 'ndcg': array([0.02778369])}
#####W######
5.51
[array([0.05962146]), array([0.03533152]), 3]
EPOCH[101/1500] loss0.332-|Sample:10.66|
[0;30;43m[TEST][0m
{'precision': array([0.01262476]), 'recall': array([0.05072597]), 'ndcg': array([0.03034644])}
#####W######
5.43
[array([0.05962146]), array([0.03533152]), 4]
EPOCH[111/1500] loss0.284-|Sample:10.56|
[0;30;43m[TEST][0m
{'precision': array([0.01208386]), 'recall': array([0.04967849]), 'ndcg': array([0.02927641])}
#####W######
5.44
[array([0.05962146]), array([0.03533152]), 5]
EPOCH[121/1500] loss0.342-|Sample:11.63|
[0;30;43m[TEST][0m
{'precision': array([0.01242883]), 'recall': array([0.05091901]), 'ndcg': array([0.03004275])}
#####W######
5.46
[array([0.05962146]), array([0.03533152]), 6]
EPOCH[131/1500] loss0.366-|Sample:10.62|
[0;30;43m[TEST][0m
{'precision': array([0.01251423]), 'recall': array([0.0509311]), 'ndcg': array([0.03012915])}
#####W######
5.46
[array([0.05962146]), array([0.03533152]), 7]
EPOCH[141/1500] loss0.321-|Sample:10.65|
[0;30;43m[TEST][0m
{'precision': array([0.01231161]), 'recall': array([0.05127353]), 'ndcg': array([0.03019304])}
#####W######
5.57
[array([0.05962146]), array([0.03533152]), 8]
EPOCH[151/1500] loss0.326-|Sample:10.60|
[0;30;43m[TEST][0m
{'precision': array([0.01121642]), 'recall': array([0.04609695]), 'ndcg': array([0.02725848])}
#####W######
5.53
[array([0.05962146]), array([0.03533152]), 9]
EPOCH[161/1500] loss0.321-|Sample:10.57|
[0;30;43m[TEST][0m
{'precision': array([0.01236185]), 'recall': array([0.05132506]), 'ndcg': array([0.02988791])}
#####W######
5.57
[array([0.05962146]), array([0.03533152]), 10]
EPOCH[171/1500] loss0.318-|Sample:10.53|
[0;30;43m[TEST][0m
{'precision': array([0.01132695]), 'recall': array([0.04637805]), 'ndcg': array([0.02718081])}
#####W######
5.62
[array([0.05962146]), array([0.03533152]), 11]
EPOCH[181/1500] loss0.292-|Sample:10.58|
[0;30;43m[TEST][0m
{'precision': array([0.01140733]), 'recall': array([0.04636689]), 'ndcg': array([0.02720737])}
#####W######
5.68
[array([0.05962146]), array([0.03533152]), 12]
EPOCH[191/1500] loss0.325-|Sample:10.61|
[0;30;43m[TEST][0m
{'precision': array([0.01104059]), 'recall': array([0.04493212]), 'ndcg': array([0.02641901])}
#####W######
5.59
[array([0.05962146]), array([0.03533152]), 13]
EPOCH[201/1500] loss0.297-|Sample:10.74|
[0;30;43m[TEST][0m
{'precision': array([0.01106069]), 'recall': array([0.04506719]), 'ndcg': array([0.02662823])}
#####W######
5.61
[array([0.05962146]), array([0.03533152]), 14]
EPOCH[211/1500] loss0.332-|Sample:10.66|
[0;30;43m[TEST][0m
{'precision': array([0.01120135]), 'recall': array([0.04588327]), 'ndcg': array([0.02703778])}
#####W######
5.51
[array([0.05962146]), array([0.03533152]), 15]
EPOCH[221/1500] loss0.333-|Sample:10.46|
[0;30;43m[TEST][0m
{'precision': array([0.01042099]), 'recall': array([0.042138]), 'ndcg': array([0.02492456])}
#####W######
5.70
[array([0.05962146]), array([0.03533152]), 16]
EPOCH[231/1500] loss0.293-|Sample:10.62|
[0;30;43m[TEST][0m
{'precision': array([0.01076931]), 'recall': array([0.04492967]), 'ndcg': array([0.02621737])}
#####W######
5.65
[array([0.05962146]), array([0.03533152]), 17]
EPOCH[241/1500] loss0.362-|Sample:10.63|
[0;30;43m[TEST][0m
{'precision': array([0.01061357]), 'recall': array([0.04425516]), 'ndcg': array([0.0259839])}
#####W######
5.68
[array([0.05962146]), array([0.03533152]), 18]
EPOCH[251/1500] loss0.308-|Sample:10.66|
[0;30;43m[TEST][0m
{'precision': array([0.01068223]), 'recall': array([0.04438645]), 'ndcg': array([0.02647018])}
#####W######
5.63
[array([0.05962146]), array([0.03533152]), 19]
EPOCH[261/1500] loss0.305-|Sample:10.68|
[0;30;43m[TEST][0m
{'precision': array([0.01063702]), 'recall': array([0.04402106]), 'ndcg': array([0.02585988])}
#####W######
5.55
[array([0.05962146]), array([0.03533152]), 20]
EPOCH[271/1500] loss0.350-|Sample:10.63|
[0;30;43m[TEST][0m
{'precision': array([0.01008272]), 'recall': array([0.0415515]), 'ndcg': array([0.0244635])}
#####W######
5.72
[array([0.05962146]), array([0.03533152]), 21]
EPOCH[281/1500] loss0.307-|Sample:10.66|
[0;30;43m[TEST][0m
{'precision': array([0.00994876]), 'recall': array([0.04080415]), 'ndcg': array([0.02401244])}
#####W######
5.59
[array([0.05962146]), array([0.03533152]), 22]
EPOCH[291/1500] loss0.311-|Sample:10.61|
[0;30;43m[TEST][0m
{'precision': array([0.01000234]), 'recall': array([0.040094]), 'ndcg': array([0.02383014])}
#####W######
5.66
[array([0.05962146]), array([0.03533152]), 23]
EPOCH[301/1500] loss0.364-|Sample:10.72|
[0;30;43m[TEST][0m
{'precision': array([0.00969422]), 'recall': array([0.03944616]), 'ndcg': array([0.02334272])}
#####W######
5.62
[array([0.05962146]), array([0.03533152]), 24]
EPOCH[311/1500] loss0.324-|Sample:10.68|
[0;30;43m[TEST][0m
{'precision': array([0.01032387]), 'recall': array([0.04268397]), 'ndcg': array([0.02525798])}
#####W######
5.60
[array([0.05962146]), array([0.03533152]), 25]
EPOCH[321/1500] loss0.323-|Sample:10.78|
[0;30;43m[TEST][0m
{'precision': array([0.00994373]), 'recall': array([0.04083113]), 'ndcg': array([0.02395092])}
#####W######
5.60
[array([0.05962146]), array([0.03533152]), 26]
EPOCH[331/1500] loss0.336-|Sample:10.66|
[0;30;43m[TEST][0m
{'precision': array([0.01060855]), 'recall': array([0.04336365]), 'ndcg': array([0.02521412])}
#####W######
5.61
[array([0.05962146]), array([0.03533152]), 27]
EPOCH[341/1500] loss0.325-|Sample:10.54|
[0;30;43m[TEST][0m
{'precision': array([0.00962221]), 'recall': array([0.0393901]), 'ndcg': array([0.0230612])}
#####W######
5.63
[array([0.05962146]), array([0.03533152]), 28]
EPOCH[351/1500] loss0.444-|Sample:10.59|
[0;30;43m[TEST][0m
{'precision': array([0.00940954]), 'recall': array([0.03842114]), 'ndcg': array([0.02260708])}
#####W######
5.55
[array([0.05962146]), array([0.03533152]), 29]
EPOCH[361/1500] loss0.367-|Sample:10.46|
[0;30;43m[TEST][0m
{'precision': array([0.00921529]), 'recall': array([0.03759513]), 'ndcg': array([0.02200061])}
#####W######
5.64
[array([0.05962146]), array([0.03533152]), 30]
EPOCH[371/1500] loss0.302-|Sample:10.61|
[0;30;43m[TEST][0m
{'precision': array([0.00837966]), 'recall': array([0.03363283]), 'ndcg': array([0.01980315])}
#####W######
5.72
[array([0.05962146]), array([0.03533152]), 31]
Beak because no update
[array([0.05962146]), array([0.03533152]), 31]
Global Function {'edgeV2', 'du_norm', 'Start'}
